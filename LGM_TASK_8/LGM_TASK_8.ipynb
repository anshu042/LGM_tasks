{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LetsGrowMore Virtual Internship Program"
      ],
      "metadata": {
        "id": "lI_G_hDIEQHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task : Next Word Prediction."
      ],
      "metadata": {
        "id": "PtSZOjFMEQP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import the required libraries"
      ],
      "metadata": {
        "id": "nVK55sNi9GV_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-HvypT2NEENp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the dataset"
      ],
      "metadata": {
        "id": "Sm0syQY_9Qdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('1661-0.txt').read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vVqE2ZXE902",
        "outputId": "9b4babd5-c9ee-43d1-cedf-8b008c8a59d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length: 581888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to split the entire dataset into each word in order without the presence of special characters."
      ],
      "metadata": {
        "id": "0Wd1PQpG9Ze7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "SSsvgtTvFRrD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, for the feature engineering part, we need to have the unique sorted words list. We also need a dictionary with each word form the unique_words list as key and its corresponding position as value."
      ],
      "metadata": {
        "id": "6Zd2LXoU9fdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "metadata": {
        "id": "BUcAoWZZFWMy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering"
      ],
      "metadata": {
        "id": "SLYtcLaP9ooD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "print(prev_words[0])\n",
        "print(next_words[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQWzoczJFbMS",
        "outputId": "37334a3f-e3f5-4bb9-d664-941c7414cc41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenberg', 's', 'the', 'adventures']\n",
            "of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create two numpy array X(for storing the features) and Y(for storing the corresponding label)."
      ],
      "metadata": {
        "id": "DGx12wc39wKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)"
      ],
      "metadata": {
        "id": "txyV5RUZFilA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1"
      ],
      "metadata": {
        "id": "RMUmMEPYFl05"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0][0])"
      ],
      "metadata": {
        "id": "mvgR0BUkGUPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d74842-b8c8-4248-86aa-28218fc2e2cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False False ... False False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the model"
      ],
      "metadata": {
        "id": "6uxsfQ4B93Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "4JJMCKpjkJQE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "V329LBq09-fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=20, shuffle=True).history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G_oqcZRkQgs",
        "outputId": "b8773e86-8ec2-41f8-c63c-40a2e42d434a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "811/811 [==============================] - 290s 353ms/step - loss: 6.0101 - accuracy: 0.1073 - val_loss: 7.0723 - val_accuracy: 0.1033\n",
            "Epoch 2/20\n",
            "811/811 [==============================] - 285s 352ms/step - loss: 5.7724 - accuracy: 0.1482 - val_loss: 7.7905 - val_accuracy: 0.1023\n",
            "Epoch 3/20\n",
            "811/811 [==============================] - 287s 354ms/step - loss: 5.7672 - accuracy: 0.1746 - val_loss: 7.9921 - val_accuracy: 0.1100\n",
            "Epoch 4/20\n",
            "811/811 [==============================] - 285s 352ms/step - loss: 5.4483 - accuracy: 0.2089 - val_loss: 8.2435 - val_accuracy: 0.1077\n",
            "Epoch 5/20\n",
            "811/811 [==============================] - 283s 349ms/step - loss: 5.1461 - accuracy: 0.2487 - val_loss: 8.2791 - val_accuracy: 0.0932\n",
            "Epoch 6/20\n",
            "811/811 [==============================] - 281s 346ms/step - loss: 4.8995 - accuracy: 0.2872 - val_loss: 8.5497 - val_accuracy: 0.0921\n",
            "Epoch 7/20\n",
            "811/811 [==============================] - 283s 349ms/step - loss: 4.6633 - accuracy: 0.3270 - val_loss: 8.7364 - val_accuracy: 0.0820\n",
            "Epoch 8/20\n",
            "811/811 [==============================] - 280s 345ms/step - loss: 4.4487 - accuracy: 0.3660 - val_loss: 8.9473 - val_accuracy: 0.0811\n",
            "Epoch 9/20\n",
            "811/811 [==============================] - 281s 346ms/step - loss: 4.2640 - accuracy: 0.4001 - val_loss: 8.9686 - val_accuracy: 0.0817\n",
            "Epoch 10/20\n",
            "811/811 [==============================] - 283s 349ms/step - loss: 4.1028 - accuracy: 0.4327 - val_loss: 9.0779 - val_accuracy: 0.0796\n",
            "Epoch 11/20\n",
            "811/811 [==============================] - 280s 345ms/step - loss: 3.9755 - accuracy: 0.4583 - val_loss: 9.2592 - val_accuracy: 0.0738\n",
            "Epoch 12/20\n",
            "811/811 [==============================] - 279s 345ms/step - loss: 3.8584 - accuracy: 0.4836 - val_loss: 9.6390 - val_accuracy: 0.0721\n",
            "Epoch 13/20\n",
            "811/811 [==============================] - 279s 344ms/step - loss: 3.7411 - accuracy: 0.5065 - val_loss: 9.5739 - val_accuracy: 0.0674\n",
            "Epoch 14/20\n",
            "811/811 [==============================] - 280s 346ms/step - loss: 3.6397 - accuracy: 0.5257 - val_loss: 9.8334 - val_accuracy: 0.0709\n",
            "Epoch 15/20\n",
            "811/811 [==============================] - 275s 338ms/step - loss: 3.5659 - accuracy: 0.5406 - val_loss: 9.9629 - val_accuracy: 0.0712\n",
            "Epoch 16/20\n",
            "811/811 [==============================] - 280s 346ms/step - loss: 3.5275 - accuracy: 0.5520 - val_loss: 9.6935 - val_accuracy: 0.0657\n",
            "Epoch 17/20\n",
            "811/811 [==============================] - 275s 339ms/step - loss: 3.5098 - accuracy: 0.5619 - val_loss: 9.7012 - val_accuracy: 0.0663\n",
            "Epoch 18/20\n",
            "811/811 [==============================] - 279s 344ms/step - loss: 3.4641 - accuracy: 0.5735 - val_loss: 9.6079 - val_accuracy: 0.0648\n",
            "Epoch 19/20\n",
            "811/811 [==============================] - 278s 343ms/step - loss: 3.4099 - accuracy: 0.5838 - val_loss: 9.7905 - val_accuracy: 0.0676\n",
            "Epoch 20/20\n",
            "811/811 [==============================] - 278s 343ms/step - loss: 3.3639 - accuracy: 0.5924 - val_loss: 9.6041 - val_accuracy: 0.0672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successful training, we will save the trained model and just load it back as needed."
      ],
      "metadata": {
        "id": "0lqMWZVH-Hpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "n3jZXM23kQjk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "MuZDFmxp-N1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        print(word)\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"It is not a lack\".lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB7w0nS9kr0m",
        "outputId": "e9573293-0656-41a8-eb68-e9d26140e315"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "not\n",
            "a\n",
            "lack\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To choose the best possible n words after the prediction from the model is done by sample function."
      ],
      "metadata": {
        "id": "YpXnGnlm-XOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "metadata": {
        "id": "kkChqBGNkxZe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, for prediction, we use the function predict_completions which use the model to predict and return the list of n predicted words."
      ],
      "metadata": {
        "id": "hIteGNik-bPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_completions(text, n=3):\n",
        "    if text == \"\":\n",
        "        return(\"0\")\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [unique_words[idx] for idx in next_indices]"
      ],
      "metadata": {
        "id": "CcAtRpC8k1J8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s see how it predicts, we use tokenizer.tokenize fo removing the punctuations and also we choose 5 first words because our predicts base on 5 previous words."
      ],
      "metadata": {
        "id": "YaviFg63-fa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q =  \"Your life will never be there in the same situation again\"\n",
        "print(\"correct sentence: \",q)\n",
        "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
        "print(\"Sequence: \",seq)\n",
        "print(\"next possible words: \", predict_completions(seq, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKXfnf34k1Tc",
        "outputId": "e04cd64b-104d-4474-f549-cbea43594006"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct sentence:  Your life will never be there in the same situation again\n",
            "Sequence:  your life will never be\n",
            "your\n",
            "life\n",
            "will\n",
            "never\n",
            "be\n",
            "next possible words:  ['and', 'at', 'that', 'very', 'too']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGuGem9V-jl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}